services:
  catalog:
    image: ghcr.io/projectnessie/nessie:0.106.0
    container_name: catalog
    depends_on:
      - storage
    ports:
      - "19120:19120"
    environment:
      NESSIE_CATALOG_DEFAULT_WAREHOUSE: warehouse
      NESSIE_CATALOG_WAREHOUSES_WAREHOUSE_LOCATION: s3://iceberg/

      NESSIE_CATALOG_SERVICE_S3_DEFAULT_OPTIONS_REGION: us-east-1
      NESSIE_CATALOG_SERVICE_S3_DEFAULT_OPTIONS_ENDPOINT: http://storage:9000
      NESSIE_CATALOG_SERVICE_S3_DEFAULT_OPTIONS_PATH_STYLE_ACCESS: "true"
      NESSIE_CATALOG_SERVICE_S3_DEFAULT_OPTIONS_AUTH_TYPE: APPLICATION_GLOBAL
      AWS_ACCESS_KEY_ID: admin
      AWS_SECRET_ACCESS_KEY: password
      AWS_REGION: us-east-1
      AWS_EC2_METADATA_DISABLED: "true"

    networks: [mnet]

  storage:
    image: minio/minio
    container_name: storage
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=password
#       - MINIO_DOMAIN=storage
      - MINIO_REGION_NAME=us-east-1
      - MINIO_REGION=us-east-1
    networks: [mnet]
    ports:
      - 9001:9001
      - 9000:9000
    command: ['server', '/data', '--console-address', ':9001']
  # Minio Client Container
  mc:
    depends_on:
      - storage
    image: minio/mc
    container_name: mc
    networks: [mnet]
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
      - AWS_DEFAULT_REGION=us-east-1
    entrypoint: >
      /bin/sh -c "
      until mc alias set minio http://storage:9000 admin password; do echo '...waiting...' && sleep 1; done;
      mc rm -r --force minio/iceberg;
      mc mb minio/iceberg;
      mc anonymous set public minio/iceberg;
      tail -f /dev/null
      "
#"


  trino-coordinator:
    image: trinodb/trino:476
    container_name: trino-coordinator
    networks: [mnet]
    ports:
      - "8080:8080"
    volumes:
      - ./trino/coordinator/config.properties:/etc/trino/config.properties
      - ./trino/catalog/iceberg.properties:/etc/trino/catalog/iceberg.properties
      - ./trino/coordinator/init.sql:/etc/trino/init.sql
    depends_on:
      - mc

  trino-worker-1:
    image: trinodb/trino:476
    container_name: trino-worker-1
    networks: [mnet]
    volumes:
      - ./trino/worker/config.properties:/etc/trino/config.properties
      - ./trino/catalog/iceberg.properties:/etc/trino/catalog/iceberg.properties
    depends_on:
      - trino-coordinator

  trino-worker-2:
    image: trinodb/trino:476
    container_name: trino-worker-2
    networks: [mnet]
    volumes:
      - ./trino/worker/config.properties:/etc/trino/config.properties
      - ./trino/catalog/iceberg.properties:/etc/trino/catalog/iceberg.properties
    depends_on:
      - trino-coordinator

  jupyter:
    image: jupyter/pyspark-notebook:latest
    ports: ["8888:8888"]
    networks: [mnet]
    depends_on:
      - catalog
      - storage
    environment:
      AWS_ACCESS_KEY_ID: admin
      AWS_SECRET_ACCESS_KEY: password
      AWS_REGION: us-east-1
      AWS_EC2_METADATA_DISABLED: "true"
      SPARK_SUBMIT_OPTS: "-Divy.cache.dir=/tmp -Divy.home=/tmp"
      #         --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,software.amazon.awssdk:bundle:2.20.131,software.amazon.awssdk:url-connection-client:2.20.131,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.106.0

    volumes:
      - ./spark/spark-defaults1.conf:/usr/local/spark/conf/spark-defaults.conf:ro

  ########

  namenode:
    image: apache/hadoop:3.3.6
    container_name: namenode
    hostname: namenode
    command: ["hdfs", "namenode"]
    ports:
      - "9870:9870"   # HDFS UI
      - "8020:8020"   # HDFS RPC
    volumes:
      - namenode:/tmp/hadoop-root/dfs/name
      - ./hadoop-conf:/opt/hadoop/etc/hadoop
    networks: [mnet]

  datanode:
    image: apache/hadoop:3.3.6
    container_name: datanode
    hostname: datanode
    command: ["hdfs", "datanode"]
    volumes:
      - datanode:/tmp/hadoop-root/dfs/data
      - ./hadoop-conf:/opt/hadoop/etc/hadoop
    depends_on: [namenode]
    networks: [mnet]

  resourcemanager:
    image: apache/hadoop:3.3.6
    container_name: resourcemanager
    hostname: resourcemanager
    command: ["yarn", "resourcemanager"]
    ports:
      - "8088:8088"   # YARN UI
    volumes:
      - ./hadoop-conf:/opt/hadoop/etc/hadoop
    depends_on: [namenode, datanode]
    networks: [mnet]

  nodemanager-1:
    image: apache/hadoop:3.3.6
    container_name: nodemanager-1
    hostname: nodemanager-1
    command: ["yarn", "nodemanager"]
    ports:
      - "8042:8042"   # NM UI (опционально)
    volumes:
      - ./hadoop-conf:/opt/hadoop/etc/hadoop
    depends_on: [resourcemanager]
    networks: [mnet]

  # контейнер, из которого ты делаешь spark-submit / spark-shell
  spark-client:
    image: apache/spark:3.5.0
    container_name: spark-client
    command: ["bash", "-lc", "tail -f /dev/null"]
    environment:
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      - YARN_CONF_DIR=/opt/hadoop/etc/hadoop
      # AWS creds для driver в client mode
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
      - AWS_EC2_METADATA_DISABLED=true
    volumes:
      - ./hadoop-conf:/opt/hadoop/etc/hadoop:ro
      - ./spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
      - ./apps:/apps
    depends_on: [resourcemanager, nodemanager-1]
    networks: [mnet]

volumes:
  namenode:
  datanode:

networks:
  mnet:
