services:
  catalog:
    image: ghcr.io/projectnessie/nessie:0.106.0
    container_name: catalog
    depends_on:
      - storage
    ports:
      - "19120:19120"
    environment:
      NESSIE_CATALOG_DEFAULT_WAREHOUSE: warehouse
      NESSIE_CATALOG_WAREHOUSES_WAREHOUSE_LOCATION: s3://iceberg/

      NESSIE_CATALOG_SERVICE_S3_DEFAULT_OPTIONS_REGION: us-east-1
      NESSIE_CATALOG_SERVICE_S3_DEFAULT_OPTIONS_ENDPOINT: http://storage:9000
      NESSIE_CATALOG_SERVICE_S3_DEFAULT_OPTIONS_PATH_STYLE_ACCESS: "true"
      NESSIE_CATALOG_SERVICE_S3_DEFAULT_OPTIONS_AUTH_TYPE: APPLICATION_GLOBAL
      AWS_ACCESS_KEY_ID: admin
      AWS_SECRET_ACCESS_KEY: password
      AWS_REGION: us-east-1
      AWS_EC2_METADATA_DISABLED: "true"

    networks: [mnet]

  storage:
    image: minio/minio
    container_name: storage
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=password
#       - MINIO_DOMAIN=storage
      - MINIO_REGION_NAME=us-east-1
      - MINIO_REGION=us-east-1
    networks: [mnet]
    ports:
      - 9001:9001
      - 9000:9000
    command: ['server', '/data', '--console-address', ':9001']
  # Minio Client Container
  mc:
    depends_on:
      - storage
    image: minio/mc
    container_name: mc
    networks: [mnet]
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
      - AWS_DEFAULT_REGION=us-east-1
    entrypoint: >
      /bin/sh -c "
      until mc alias set minio http://storage:9000 admin password; do echo '...waiting...' && sleep 1; done;
      mc rm -r --force minio/iceberg;
      mc mb minio/iceberg;
      mc anonymous set public minio/iceberg;
      tail -f /dev/null
      "
#"


  trino-coordinator:
    image: trinodb/trino:476
    container_name: trino-coordinator
    networks: [mnet]
    ports:
      - "8080:8080"
    volumes:
      - ./trino/coordinator/config.properties:/etc/trino/config.properties
      - ./trino/catalog/iceberg.properties:/etc/trino/catalog/iceberg.properties
      - ./trino/coordinator/init.sql:/etc/trino/init.sql
    depends_on:
      - mc

  trino-worker-1:
    image: trinodb/trino:476
    container_name: trino-worker-1
    networks: [mnet]
    volumes:
      - ./trino/worker/config.properties:/etc/trino/config.properties
      - ./trino/catalog/iceberg.properties:/etc/trino/catalog/iceberg.properties
    depends_on:
      - trino-coordinator

  trino-worker-2:
    image: trinodb/trino:476
    container_name: trino-worker-2
    networks: [mnet]
    volumes:
      - ./trino/worker/config.properties:/etc/trino/config.properties
      - ./trino/catalog/iceberg.properties:/etc/trino/catalog/iceberg.properties
    depends_on:
      - trino-coordinator

  spark-master:
    image: apache/spark:3.5.0
    container_name: spark-master
    command: bash -c "/opt/spark/sbin/start-master.sh && tail -f /dev/null"
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=0.0.0.0
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      # creds for the AWS SDK inside the driver (if you run spark-submit here)
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
      - AWS_EC2_METADATA_DISABLED=true
    volumes:
      - ./spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro

    ports:
      - "8090:8080"
      - "7077:7077"
    networks: [mnet]

  spark-worker-1:
    image: apache/spark:3.5.0
    container_name: spark-worker-1
    command: bash -c "/opt/spark/sbin/start-worker.sh spark://spark-master:7077 && tail -f /dev/null"
    environment:
      - SPARK_MODE=worker
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
      - AWS_EC2_METADATA_DISABLED=true
    volumes:
      - ./spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
    networks: [mnet]
    depends_on: [spark-master]

  spark-worker-2:
    image: apache/spark:3.5.0
    container_name: spark-worker-2
    command: bash -c "/opt/spark/sbin/start-worker.sh spark://spark-master:7077 && tail -f /dev/null"
    environment:
      - SPARK_MODE=worker
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
      - AWS_EC2_METADATA_DISABLED=true
    volumes:
      - ./spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
    networks: [mnet]
    depends_on: [spark-master]

  jupyter:
    image: jupyter/pyspark-notebook:latest
    ports: ["8888:8888"]
    networks: [mnet]
    depends_on:
      - spark-master
      - catalog
      - storage
    environment:
      AWS_ACCESS_KEY_ID: admin
      AWS_SECRET_ACCESS_KEY: password
      AWS_REGION: us-east-1
      AWS_EC2_METADATA_DISABLED: "true"
      SPARK_SUBMIT_OPTS: "-Divy.cache.dir=/tmp -Divy.home=/tmp"
      #         --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,software.amazon.awssdk:bundle:2.20.131,software.amazon.awssdk:url-connection-client:2.20.131,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.106.0

    volumes:
      - ./spark/spark-defaults1.conf:/usr/local/spark/conf/spark-defaults.conf:ro

volumes:
  spark_ivy:
  spark-data:
  spark-apps:

networks:
  mnet:
