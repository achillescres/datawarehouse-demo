# Iceberg + REST catalog
spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.defaultCatalog=nessie

spark.sql.catalog.nessie=org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.nessie.type=rest
spark.sql.catalog.nessie.uri=http://nessie:19120/iceberg/main/
spark.sql.catalog.nessie.warehouse=s3://warehouse/

# Packages (Iceberg runtime + S3 support)
spark.jars.packages=org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.8.1,org.apache.iceberg:iceberg-aws-bundle:1.8.1,software.amazon.awssdk:bundle:2.24.8,software.amazon.awssdk:url-connection-client:2.24.8

# If you want Spark to use S3A directly (not strictly required if Nessie vends config)
spark.hadoop.fs.s3a.endpoint=http://minio:9000
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.connection.ssl.enabled=false
spark.hadoop.fs.s3a.access.key=admin
spark.hadoop.fs.s3a.secret.key=password
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.jars.ivy=/tmp/ivy

# Use HDFS for Spark staging / scratch
spark.yarn.stagingDir=hdfs:///tmp/hadoop-yarn/staging

# Put event logs somewhere writable (optional but useful)
spark.eventLog.enabled=true
spark.eventLog.dir=hdfs:///spark-events

# Avoid writing warehouse to HDFS root by accident
spark.sql.warehouse.dir=hdfs:///user/spark/warehouse

